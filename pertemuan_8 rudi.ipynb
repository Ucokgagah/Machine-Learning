{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHk6x_NKasy4"
      },
      "source": [
        "# Pertemuan 09 - Optimalisasi Kinerja Model Machine Learning\n",
        "\n",
        "## Capaian Pembelajaran:\n",
        "> 3.3: Mampu memaksimalkan kinerja model machine learning dengan menerapkan teknik-teknik optimasi seperti hyperparameter tuning, feature engineering, dan ensemble methods.\n",
        "\n",
        "## Pokok Bahasan\n",
        "1. Definisi, dampak, dan pentingnya optimalisasi model\n",
        "2. Hyperparameter Tuning\n",
        "3. Grid search\n",
        "4. Random search\n",
        "5. Bayesian optimization\n",
        "6. Feature selection\n",
        "7. Feature extraction\n",
        "8. Polynomial features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTMOqmjcasy5"
      },
      "source": [
        "## 1. Definisi, Dampak, dan Pentingnya Optimalisasi Model\n",
        "Optimalisasi model dalam machine learning adalah proses menyesuaikan parameter dan hiperparameter model untuk meningkatkan kinerja model dalam membuat prediksi atau pengklasifikasian. Tujuan dari optimalisasi model adalah untuk menemukan konfigurasi terbaik yang memaksimalkan akurasi atau performa model sesuai dengan metrik evaluasi yang dipilih. Ini sering melibatkan teknik seperti grid search, random search, bayesian optimization, dan lainnya.\n",
        "\n",
        "### Dampak Optimalisasi Model\n",
        "\n",
        "#### a. Meningkatkan Akurasi dan Kinerja\n",
        "Optimalisasi model dapat secara signifikan meningkatkan akurasi prediksi atau klasifikasi, menghasilkan model yang lebih andal dan efektif. Misalnya, dengan menyesuaikan parameter seperti jumlah pohon dalam random forest atau learning rate dalam gradient boosting, kinerja model dapat ditingkatkan.\n",
        "\n",
        "#### b. Mengurangi Kesalahan:\n",
        "Dengan mengoptimalkan model, tingkat kesalahan seperti false positives dan false negatives dapat dikurangi, yang sangat penting dalam aplikasi kritis seperti deteksi penipuan atau diagnosis medis.Optimalisasi dapat membantu menemukan keseimbangan yang tepat antara bias dan varians, mengurangi risiko overfitting atau underfitting.\n",
        "\n",
        "#### c. Efisiensi Komputasi:\n",
        "Proses optimalisasi juga dapat mengarah pada penggunaan sumber daya komputasi yang lebih efisien, dengan menemukan konfigurasi parameter yang memberikan kinerja terbaik dengan biaya komputasi yang lebih rendah. Optimalisasi hyperparameter dapat membantu mengurangi waktu pelatihan dan inferensi.\n",
        "\n",
        "\n",
        "### Pentingnya Optimalisasi Model\n",
        "#### a. Meningkatkan Keandalan Model\n",
        "Optimalisasi model memastikan bahwa model dapat diandalkan dalam berbagai kondisi dan tidak hanya bekerja dengan baik pada data pelatihan tetapi juga pada data yang belum pernah dilihat sebelumnya. Dengan menguji berbagai kombinasi parameter, optimalisasi membantu mengidentifikasi set parameter yang memberikan kinerja konsisten.\n",
        "\n",
        "#### b. Menghadapi Tantangan di Dunia Nyata\n",
        "Dalam aplikasi dunia nyata, data seringkali tidak sempurna dan memiliki ketidakseimbangan kelas, noise, atau outliers. Optimalisasi model membantu model beradaptasi dan bekerja dengan baik dalam kondisi yang beragam ini. Misalnya, dalam tugas klasifikasi medis, optimalisasi dapat memastikan bahwa model memberikan hasil yang akurat meskipun ada variasi dalam data pasien.\n",
        "\n",
        "#### c. Mendukung Pengambilan Keputusan:\n",
        "Model yang dioptimalkan memberikan prediksi yang lebih akurat dan dapat diandalkan, yang sangat penting dalam pengambilan keputusan bisnis, seperti rekomendasi produk, analisis risiko, dan manajemen rantai pasokan. Optimalisasi model dapat membantu bisnis mengidentifikasi peluang dan risiko dengan lebih baik, mendukung strategi yang lebih efektif.\n",
        "___\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dz1WFo4Hasy6"
      },
      "source": [
        "## 2. Hyperparameter Tuning\n",
        "Hyperparameter tuning adalah proses memilih set parameter terbaik untuk model machine learning untuk meningkatkan kinerjanya. Berbeda dengan parameter model yang dipelajari selama pelatihan (misalnya, bobot dalam regresi linier), hyperparameter adalah pengaturan yang harus ditentukan sebelum pelatihan model, seperti learning rate dalam neural network, jumlah pohon dalam random forest, atau nilai k dalam k-nearest neighbors (KNN).\n",
        "\n",
        "### Pentingnya Hyperparameter Tuning\n",
        "1. Meningkatkan Akurasi Model: Hyperparameter yang tepat dapat meningkatkan akurasi model secara signifikan. Model dengan set hyperparameter yang dioptimalkan akan lebih baik dalam memprediksi data yang belum pernah dilihat sebelumnya.\n",
        "2. Menghindari Overfitting dan Underfitting: Hyperparameter tuning membantu dalam menemukan keseimbangan yang tepat antara bias dan varians. Ini membantu menghindari overfitting (model terlalu sesuai dengan data pelatihan) dan underfitting (model tidak cukup belajar dari data pelatihan).\n",
        "3. Efisiensi Komputasi: Dengan memilih hyperparameter yang optimal, Anda dapat meningkatkan efisiensi komputasi, mengurangi waktu pelatihan, dan penggunaan sumber daya yang lebih efisien."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba6a_oB6asy6"
      },
      "source": [
        "## 3. Grid Search\n",
        "Grid search adalah metode yang secara sistematis menjelajahi ruang hyperparameter yang telah ditentukan oleh pengguna dengan mencoba setiap kombinasi yang mungkin. Proses: Mengatur grid dari parameter, melatih model pada setiap kombinasi, dan memilih kombinasi dengan kinerja terbaik berdasarkan metrik evaluasi yang dipilih. Kelebihan: Memberikan jaminan menemukan kombinasi parameter yang optimal dalam ruang pencarian yang ditentukan. Kekurangan: Memerlukan waktu komputasi yang sangat besar, terutama jika ruang pencarian parameter besar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rDvQOC4sasy6",
        "outputId": "9f0cfedd-af99-43c9-d809-899f7dde573b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 10, 'n_estimators': 200}\n",
            "Best Score: 0.9583333333333334\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split # Import train_test_split\n",
        "from sklearn.datasets import load_iris # Import a sample dataset\n",
        "\n",
        "# Load a sample dataset (e.g., Iris dataset)\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# Definisikan model dan parameter grid\n",
        "model = RandomForestClassifier()\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20, 30]\n",
        "}\n",
        "\n",
        "# Grid Search\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Best Score:\", grid_search.best_score_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qo6YFEw4asy7"
      },
      "source": [
        "## 4. Random Search\n",
        "Definisi: Random search memilih kombinasi hyperparameter secara acak dari ruang pencarian yang telah ditentukan pengguna Proses: Mengatur distribusi dari parameter, melatih model pada sejumlah kombinasi acak, dan memilih kombinasi dengan kinerja terbaik berdasarkan metrik evaluasi yang dipilih. Kelebihan: Lebih efisien daripada grid search, terutama pada ruang pencarian parameter yang besar. Kekurangan: Tidak memberikan jaminan menemukan kombinasi parameter yang optimal tetapi bisa mendekati hasil optimal dengan waktu komputasi lebih sedikit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "aju5bHELasy7",
        "outputId": "a5f828a9-19e5-4c53-de98-d47ec8f071a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'n_estimators': 100, 'max_depth': 10, 'bootstrap': True}\n",
            "Best Score: 0.95\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Definisikan model dan parameter distribusi\n",
        "model = RandomForestClassifier()\n",
        "param_dist = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'bootstrap': [True, False]\n",
        "}\n",
        "\n",
        "# Random Search\n",
        "random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=10, cv=5, scoring='accuracy')\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", random_search.best_params_)\n",
        "print(\"Best Score:\", random_search.best_score_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DG-nI61Yasy8"
      },
      "source": [
        "## 5. Bayesian Optimization\n",
        "Definisi: Bayesian optimization menggunakan model probabilistik untuk memilih hyperparameter yang diharapkan menghasilkan kinerja terbaik berdasarkan evaluasi sebelumnya. Proses: Menggunakan proses Gaussian atau model lainnya untuk memperkirakan distribusi dari fungsi tujuan dan memilih hyperparameter yang memaksimalkan ekspektasi perbaikan. Kelebihan: Lebih efisien dalam menemukan hyperparameter optimal dibandingkan grid search dan random search. Kekurangan: Memerlukan pemahaman yang lebih dalam tentang statistik dan probabilitas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "HeXeOB7Xasy8",
        "outputId": "881f4011-6271-4cb0-eed4-387b57cacba7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-optimize in /usr/local/lib/python3.11/dist-packages (0.10.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (1.5.1)\n",
            "Requirement already satisfied: pyaml>=16.9 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (25.5.0)\n",
            "Requirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (1.15.3)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (1.6.1)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (24.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from pyaml>=16.9->scikit-optimize) (6.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0.0->scikit-optimize) (3.6.0)\n",
            "Best Parameters: OrderedDict([('bootstrap', True), ('max_depth', 27), ('n_estimators', 108)])\n",
            "Best Score: 0.95\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from skopt import BayesSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load a sample dataset (e.g., Iris dataset)\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Definisikan model dan parameter distribusi\n",
        "model = RandomForestClassifier()\n",
        "param_dist = {\n",
        "    'n_estimators': (50, 200),\n",
        "    'max_depth': (10, 30),\n",
        "    'bootstrap': [True, False]\n",
        "}\n",
        "\n",
        "# Bayesian Optimization\n",
        "bayes_search = BayesSearchCV(estimator=model, search_spaces=param_dist, n_iter=10, cv=5, scoring='accuracy')\n",
        "bayes_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", bayes_search.best_params_)\n",
        "print(\"Best Score:\", bayes_search.best_score_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RBGgeEUasy8"
      },
      "source": [
        "## 6. Feature Selection\n",
        "Feature selection adalah proses memilih subset fitur (variabel) yang paling relevan dari data untuk digunakan dalam membangun model machine learning. Tujuannya adalah untuk meningkatkan kinerja model dengan mengurangi dimensi data, mengurangi risiko overfitting, mempercepat waktu pelatihan, dan meningkatkan interpretabilitas model.\n",
        "\n",
        "### Pentingnya Feature Selection\n",
        "1. Meningkatkan Kinerja Model: Dengan memilih fitur yang paling relevan, model dapat memfokuskan perhatian pada informasi yang paling berguna, meningkatkan akurasi dan mengurangi noise yang dapat mempengaruhi hasil prediksi.\n",
        "2. Mengurangi Overfitting: Menggunakan terlalu banyak fitur dapat membuat model terlalu rumit dan mudah overfit pada data pelatihan. Feature selection membantu mengurangi kompleksitas model dengan hanya menggunakan fitur yang penting.\n",
        "3. Mempercepat Waktu Pelatihan: Mengurangi jumlah fitur dapat mempercepat proses pelatihan dan inferensi model, terutama untuk dataset besar dengan banyak fitur.\n",
        "4. Meningkatkan Interpretabilitas: Dengan mengurangi jumlah fitur, model menjadi lebih mudah dipahami dan diinterpretasikan, yang penting dalam aplikasi di mana transparansi model sangat penting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j61WjADcasy9"
      },
      "source": [
        "### Teknik-Teknik Feature Selection\n",
        "#### a. Filter Methods\n",
        "Definisi: Teknik filter memilih fitur berdasarkan statistik atau metrik individu sebelum proses pelatihan model. Ini dilakukan tanpa mempertimbangkan model machine learning. Teknik Umum: Chi-Square Test: Mengukur independensi antara fitur dan target. Correlation Coefficient: Mengukur hubungan linier antara fitur dan target. Mutual Information: Mengukur ketergantungan antara fitur dan target."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "zbtur-Q0asy9",
        "outputId": "cd3b9547-557a-47c3-fa7c-d903a2448b4f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected features: ['petal length (cm)', 'petal width (cm)']\n"
          ]
        }
      ],
      "source": [
        "# Contoh Implementasi (Chi-Square Test)\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn.datasets import load_iris\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "# Chi-Square feature selection\n",
        "selector = SelectKBest(score_func=chi2, k=2)\n",
        "X_new = selector.fit_transform(X, y)\n",
        "\n",
        "print(\"Selected features:\", X.columns[selector.get_support()].tolist())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeYEFvG7asy9"
      },
      "source": [
        "#### b. Wrapper Methods\n",
        "Definisi: Teknik wrapper memilih fitur berdasarkan kinerja model machine learning. Ini melibatkan pelatihan model berulang kali dengan subset fitur yang berbeda untuk menemukan yang terbaik. Teknik Umum: Recursive Feature Elimination (RFE): Menghapus fitur terburuk secara iteratif dan membangun model hingga fitur terbaik ditemukan. Forward Selection: Memulai dengan fitur kosong dan menambahkan fitur satu per satu berdasarkan peningkatan kinerja model. Backward Elimination: Memulai dengan semua fitur dan menghapus fitur satu per satu berdasarkan penurunan kinerja model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "VL8P_yxiasy-",
        "outputId": "f9896cef-51a9-4410-dd74-3c95f3472b47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected features: [False False  True  True]\n",
            "Feature ranking: [3 2 1 1]\n"
          ]
        }
      ],
      "source": [
        "# Contoh Implementasi (Recursive Feature Elimination)\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Recursive Feature Elimination\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "rfe = RFE(model, n_features_to_select=2)\n",
        "fit = rfe.fit(X, y)\n",
        "\n",
        "print(\"Selected features:\", fit.support_)\n",
        "print(\"Feature ranking:\", fit.ranking_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yz5YRYLNasy-"
      },
      "source": [
        "### c. Embedded Methods\n",
        "Definisi: Teknik embedded memilih fitur selama proses pelatihan model, menggabungkan seleksi fitur dan pelatihan model dalam satu langkah. Teknik Umum: Lasso Regression (L1 Regularization): Memilih fitur dengan memberikan penalti pada koefisien regresi, membuat beberapa koefisien menjadi nol. Decision Trees: Menggunakan pentingnya fitur yang dihitung selama pelatihan model decision tree."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "y0cmuq5kasy-",
        "outputId": "e4475452-bfe8-45f3-9cc8-d841ad147929",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected features: ['petal length (cm)']\n"
          ]
        }
      ],
      "source": [
        "# Contoh Implementasi (Lasso Regression):\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.datasets import load_iris\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "# Lasso feature selection\n",
        "model = Lasso(alpha=0.1)\n",
        "model.fit(X, y)\n",
        "\n",
        "print(\"Selected features:\", X.columns[model.coef_ != 0].tolist())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jq9Xo2UQasy-"
      },
      "source": [
        "## 7. Feature Extraction\n",
        "Feature extraction adalah proses mengubah data mentah menjadi fitur yang dapat digunakan oleh model machine learning. Tujuan dari feature extraction adalah untuk menyederhanakan representasi data tanpa kehilangan informasi penting. Ini sangat berguna ketika bekerja dengan data berdimensi tinggi, seperti teks, gambar, atau sinyal, di mana transformasi ke dalam fitur yang bermakna dapat meningkatkan kinerja model dan mempermudah interpretasi.\n",
        "\n",
        "\n",
        "### Pentingnya Feature Extraction\n",
        "1. Mengurangi Dimensi Data: Feature extraction membantu mengurangi jumlah fitur yang diperlukan, mengurangi beban komputasi dan mempercepat pelatihan model.\n",
        "2. Meningkatkan Kinerja Model: Dengan mengekstrak fitur yang relevan dan bermakna, model dapat bekerja lebih baik dalam memprediksi atau mengklasifikasikan data, karena data yang tidak relevan atau berisik telah dikurangi.\n",
        "3. Menyederhanakan Model: Model yang dibangun dari fitur yang diekstraksi seringkali lebih sederhana dan lebih mudah diinterpretasikan, yang penting untuk aplikasi yang membutuhkan transparansi.\n",
        "4. Menangani Data Berdimensi Tinggi: Dalam kasus data berdimensi tinggi seperti gambar atau teks, feature extraction membantu merangkum informasi penting ke dalam bentuk yang lebih terstruktur dan ringkas.\n",
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtiKEc9Uasy-"
      },
      "source": [
        "### Teknik-Teknik Feature Extraction\n",
        "\n",
        "### Principal Component Analysis (PCA)\n",
        "Definisi: PCA adalah teknik statistik yang mengubah data mentah menjadi komponen utama yang merupakan kombinasi linier dari variabel asli dengan varians maksimum. Proses: Mengurangi dimensi data dengan memproyeksikan data ke ruang baru yang terdiri dari komponen utama. Contoh Implementasi:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "TgsK63yMasy-",
        "outputId": "19d1e3ef-a4a9-43b2-c432-ee3e7bce0ed6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explained Variance Ratio: [0.92461872 0.05306648]\n",
            "PCA Components:\n",
            " [[ 0.36138659 -0.08452251  0.85667061  0.3582892 ]\n",
            " [ 0.65658877  0.73016143 -0.17337266 -0.07548102]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "\n",
        "# PCA\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "print(\"Explained Variance Ratio:\", pca.explained_variance_ratio_)\n",
        "print(\"PCA Components:\\n\", pca.components_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FO2bL7xgasy-"
      },
      "source": [
        "### Linear Discriminant Analysis (LDA)\n",
        "Definisi: LDA adalah teknik yang mencari kombinasi linear dari fitur yang memaksimalkan separasi antar kelas. Proses: Mengurangi dimensi data dengan memproyeksikan data ke ruang yang memaksimalkan jarak antara kelas. Contoh Implementasi:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "izaeBpYoasy_",
        "outputId": "e3b36783-c853-403c-c798-dfa09bbc86a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explained Variance Ratio: [0.9912126 0.0087874]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# LDA\n",
        "lda = LinearDiscriminantAnalysis(n_components=2)\n",
        "X_lda = lda.fit_transform(X, y)\n",
        "\n",
        "print(\"Explained Variance Ratio:\", lda.explained_variance_ratio_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAV57QHRasy_"
      },
      "source": [
        "### Bag of Words (BoW)\n",
        "Definisi: BoW adalah teknik representasi teks di mana teks diubah menjadi vektor yang menghitung frekuensi kemunculan kata Proses: Mengubah teks menjadi representasi vektor yang dapat digunakan oleh model machine learning. Contoh Implementasi:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "7ZigwJbKasy_",
        "outputId": "6bafd98b-6575-4596-9ce0-e97d7f493616",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Names: ['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n",
            "Bag of Words:\n",
            " [[0 1 1 1 0 0 1 0 1]\n",
            " [0 2 0 1 0 1 1 0 1]\n",
            " [1 0 0 1 1 0 1 1 1]\n",
            " [0 1 1 1 0 0 1 0 1]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Contoh data teks\n",
        "corpus = [\n",
        "    \"This is the first document.\",\n",
        "    \"This document is the second document.\",\n",
        "    \"And this is the third one.\",\n",
        "    \"Is this the first document?\"\n",
        "]\n",
        "\n",
        "# Bag of Words\n",
        "vectorizer = CountVectorizer()\n",
        "X_bow = vectorizer.fit_transform(corpus)\n",
        "\n",
        "print(\"Feature Names:\", vectorizer.get_feature_names_out())\n",
        "print(\"Bag of Words:\\n\", X_bow.toarray())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAhryCQHasy_"
      },
      "source": [
        "### Term Frequency-Inverse Document Frequency (TF-IDF)\n",
        "Definisi: TF-IDF adalah teknik yang memberi bobot pada kata-kata dalam teks berdasarkan frekuensi kemunculannya dalam dokumen dan seberapa jarang kata tersebut muncul di seluruh dokumen. Proses: Mengubah teks menjadi representasi vektor yang mempertimbangkan pentingnya kata dalam konteks dokumen dan korpus. Contoh Implementasi:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "kG6PoHqhasy_",
        "outputId": "c8946c62-4144-43f0-d98f-16c1ff533cd0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Names: ['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n",
            "TF-IDF:\n",
            " [[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
            "  0.38408524 0.         0.38408524]\n",
            " [0.         0.6876236  0.         0.28108867 0.         0.53864762\n",
            "  0.28108867 0.         0.28108867]\n",
            " [0.51184851 0.         0.         0.26710379 0.51184851 0.\n",
            "  0.26710379 0.51184851 0.26710379]\n",
            " [0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
            "  0.38408524 0.         0.38408524]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Contoh data teks\n",
        "corpus = [\n",
        "    \"This is the first document.\",\n",
        "    \"This document is the second document.\",\n",
        "    \"And this is the third one.\",\n",
        "    \"Is this the first document?\"\n",
        "]\n",
        "\n",
        "# TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_tfidf = vectorizer.fit_transform(corpus)\n",
        "\n",
        "print(\"Feature Names:\", vectorizer.get_feature_names_out())\n",
        "print(\"TF-IDF:\\n\", X_tfidf.toarray())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ln-_LVQYasy_"
      },
      "source": [
        "### Convolutional Neural Networks (CNNs) for Images\n",
        "Definisi: CNN adalah jenis neural network yang dirancang untuk memproses data grid seperti gambar.\n",
        "Proses: Menggunakan lapisan konvolusi untuk mengekstrak fitur dari gambar, seperti tepi, tekstur, dan objek.\n",
        "Contoh Implementasi:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "sej6ZNJdasy_",
        "outputId": "b6750abb-58e4-4391-cf6d-42baff6aa57b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 344
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m36,928\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m56,320\u001b[0m (220.00 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">56,320</span> (220.00 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m56,320\u001b[0m (220.00 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">56,320</span> (220.00 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Contoh model CNN sederhana\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "\n",
        "# Menampilkan arsitektur model\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hw17rI9MaszA"
      },
      "source": [
        "## 8. Polynomial Features\n",
        "Polynomial features adalah teknik transformasi yang menambah fitur baru ke dataset dengan menghitung semua kombinasi polinomial dari fitur yang ada hingga derajat tertentu. Teknik ini memungkinkan model untuk menangkap hubungan non-linier antara fitur, yang bisa meningkatkan kemampuan prediktif model dalam beberapa kasus.\n",
        "\n",
        "### Tujuan Polynomial Features\n",
        "1. Menangkap Hubungan Non-Linear: Polynomial features memungkinkan model linear untuk menangkap hubungan non-linear antara fitur dan target, yang meningkatkan fleksibilitas dan kinerja model.\n",
        "2. Meningkatkan Kinerja Model: Dengan menambah fitur baru yang merepresentasikan interaksi dan eksponen dari fitur asli, model bisa menjadi lebih kuat dalam menangkap pola yang kompleks dalam data.\n",
        "3. Meningkatkan Kekuatan Prediktif: Polynomial features bisa membantu model untuk lebih baik dalam memprediksi data yang memiliki hubungan non-linear, yang tidak bisa ditangkap oleh model linear sederhana.\n",
        "_____"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMU6VAP-aszA"
      },
      "source": [
        "### Teknik-Teknik Polynomial Features\n",
        "PolynomialFeatures dari scikit-learn. Definisi: PolynomialFeatures adalah kelas dalam scikit-learn yang digunakan untuk menghasilkan fitur polinomial dari dataset yang ada. Proses: Menghitung semua kombinasi polinomial dari fitur yang ada hingga derajat tertentu. Contoh Implementasi:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "pE0Ms6IFaszA",
        "outputId": "a310c2bc-518f-47ce-9747-0294ef2a318c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Feature1  Feature2  Feature1^2  Feature1 Feature2  Feature2^2\n",
            "0       1.0       4.0         1.0                4.0        16.0\n",
            "1       2.0       5.0         4.0               10.0        25.0\n",
            "2       3.0       6.0         9.0               18.0        36.0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "# Contoh dataset\n",
        "data = {\n",
        "    'Feature1': [1, 2, 3],\n",
        "    'Feature2': [4, 5, 6]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Polynomial Features\n",
        "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
        "poly_features = poly.fit_transform(df)\n",
        "\n",
        "# Menampilkan fitur polinomial\n",
        "poly_feature_names = poly.get_feature_names_out(input_features=df.columns)\n",
        "df_poly = pd.DataFrame(poly_features, columns=poly_feature_names)\n",
        "\n",
        "print(df_poly)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApEHuz5SaszA"
      },
      "source": [
        "Dalam contoh di atas, kita menggunakan PolynomialFeatures dari scikit-learn untuk menghasilkan fitur polinomial dari dataset yang memiliki dua fitur (Feature1 dan Feature2). Dengan menetapkan derajat polinomial ke 2, PolynomialFeatures akan menghasilkan fitur baru yang mencakup semua kombinasi linier, kuadrat, dan produk silang dari fitur asli."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTRsGKOSaszA"
      },
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}